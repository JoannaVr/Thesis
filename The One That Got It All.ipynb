{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data from the csv file\n",
    "data = pd.read_csv('emotions.csv')\n",
    "X = data.drop(\"label\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data example\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the class from labels to numbers (0, 1, 2)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['label']=le.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array only for the class\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot for each time series\n",
    "values = data.loc[:,'fft_0_b':'fft_9_b'].values\n",
    "pyplot.figure()\n",
    "for i in range(values.shape[1]):\n",
    "    pyplot.subplot(values.shape[1], 1, i+1)\n",
    "    pyplot.plot(values[:, i])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the train function and spliting the data to train and test sets randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the class labels from numbers to binary\n",
    "# example:                     0 1 2\n",
    "# 1009    2             1009   0 0 1\n",
    "# 1150    1      ->     1150   0 1 0\n",
    "# 1460    0             1460   1 0 0\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories distribution\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------- MultiLayer Perceptron ------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "test_k_values = range(1, 261)\n",
    "loss = []\n",
    "test_k_values_accuracy = []\n",
    "train_k_values_accuracy = []\n",
    "\n",
    "# calling the classifier\n",
    "test_mlp = MLPClassifier(hidden_layer_sizes=(7000,100), activation='logistic', solver='sgd',\n",
    "            learning_rate_init=0.01, learning_rate='adaptive', max_iter=260, verbose=False,\n",
    "            random_state=1, alpha=0.01, n_iter_no_change=10, tol=1e-4, batch_size=30)\n",
    "\n",
    "# evaluating accuracy and loss\n",
    "for k in test_k_values:\n",
    "    loss.append(test_mlp.partial_fit(X_train, y_train, np.unique(y)).loss_)\n",
    "    test_k_values_accuracy.append(test_mlp.score(X_test, y_test))\n",
    "    train_k_values_accuracy.append(test_mlp.score(X_train, y_train))\n",
    "    k=k+1\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(test_k_values, train_k_values_accuracy, '-', label=\"Training Accuracy\")\n",
    "plt.plot(test_k_values, test_k_values_accuracy, ':', label=\"Validation Accuracy\")\n",
    "plt.title('MLP - Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(test_k_values, loss, '-', label=\"Loss\")\n",
    "plt.title('MLP - Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of the algorithm form the training set\n",
    "y_pred = test_mlp.predict(X_train)\n",
    "model_acc_train = accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of the algorithm form the testing set\n",
    "y_pred = test_mlp.predict(X_test)\n",
    "model_acc_test = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP - Training Accuracy: %.1f\" %  (model_acc_train*100))\n",
    "print(\"MLP - Testing Accuracy: %.1f\" % (model_acc_test*100))\n",
    "print(\"MLP - Loss: %.2f\" % loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------- Keras Model -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of input features\n",
    "Keras_n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# define Keras model\n",
    "Keras_model = Sequential()\n",
    "# adding dense layers\n",
    "Keras_model.add(Dense(512, input_shape=Keras_n_features[1:]))\n",
    "Keras_model.add(Dense(512, activation='sigmoid'))\n",
    "Keras_model.add(Dense(512, activation='sigmoid'))\n",
    "# adding output layer\n",
    "Keras_model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# calculating early stopping and learning rate scheduler\n",
    "Keras_es = EarlyStopping(monitor='val_loss', mode='min', verbose=10, patience=10)\n",
    "Keras_lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch:0.01 * np.exp(-epoch / 10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# compiling the model\n",
    "Keras_opt_adam = optimizers.Adam(learning_rate=0.01)\n",
    "Keras_model.compile(optimizer=Keras_opt_adam, loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "Keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model image\n",
    "tf.keras.utils.plot_model(Keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "Keras_hist = Keras_model.fit(X_train, y_train, epochs=30, batch_size=50, verbose=0, \n",
    "                             validation_data=(X_test, y_test), callbacks=[Keras_es, Keras_lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    " \n",
    "Keras_acc = Keras_hist.history['accuracy']\n",
    "Keras_val = Keras_hist.history['val_accuracy']\n",
    "Keras_epochs = range(1, len(Keras_acc) + 1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(Keras_epochs, Keras_acc, '-', label='Training accuracy')\n",
    "plt.plot(Keras_epochs, Keras_val, ':', label='Validation accuracy')\n",
    "plt.title('Keras - Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "Keras_loss = Keras_hist.history['loss']\n",
    "Keras_val_loss = Keras_hist.history['val_loss']\n",
    "\n",
    "plt.plot(Keras_epochs, Keras_loss, '-', label='Loss')\n",
    "plt.plot(Keras_epochs, Keras_val_loss, ':', label='Validation loss')\n",
    "plt.title('Keras - Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "Keras_model_acc_train = Keras_model.evaluate(X_train, y_train,verbose=0)\n",
    "Keras_model_acc_test = Keras_model.evaluate(X_test, y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keras - Training Accuracy: %.1f\" % (Keras_model_acc_train[1]*100))\n",
    "print(\"Keras - Testing Accuracy: %.1f\" % (Keras_model_acc_test[1]*100))\n",
    "print(\"Keras - Loss: %.2f\" % Keras_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------- Long Short-Term Memory -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the dimensions from 2D to 3D as needed for the LSTM, GRU amd CNN models\n",
    "X_train = np.array(X_train).reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "X_test = np.array(X_test).reshape((X_test.shape[0],X_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of input features\n",
    "n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# define LSTM model\n",
    "LSTM_model = Sequential()\n",
    "# adding LSTM layer\n",
    "LSTM_model.add(LSTM(3, input_shape=n_features[1:], return_sequences=True))\n",
    "# adding fully connected layer\n",
    "LSTM_model.add(Flatten())\n",
    "# adding output layer\n",
    "LSTM_model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating early stopping and learning rate scheduler\n",
    "LSTM_es = EarlyStopping(monitor='val_loss', mode='min', verbose=10, patience=10)\n",
    "LSTM_lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "LSTM_opt_adam = optimizers.Adam(learning_rate=0.001)\n",
    "LSTM_model.compile(optimizer=LSTM_opt_adam, loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model image\n",
    "tf.keras.utils.plot_model(LSTM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "LSTM_hist = LSTM_model.fit(X_train, y_train, epochs=5, batch_size=50, verbose=0, \n",
    "                 validation_data=(X_test, y_test), callbacks=[LSTM_es, LSTM_lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_acc = LSTM_hist.history['accuracy']\n",
    "LSTM_val = LSTM_hist.history['val_accuracy']\n",
    "LSTM_epochs = range(1, len(LSTM_acc) + 1)\n",
    "\n",
    "# summarize hisrory for accuracy\n",
    "plt.plot(LSTM_epochs, LSTM_acc, '-', label='Training accuracy')\n",
    "plt.plot(LSTM_epochs, LSTM_val, ':', label='Validation accuracy')\n",
    "plt.title('LSTM - Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "LSTM_loss = LSTM_hist.history['loss']\n",
    "LSTM_val_loss = LSTM_hist.history['val_loss']\n",
    "\n",
    "plt.plot(LSTM_epochs, LSTM_loss, '-', label='Loss')\n",
    "plt.plot(LSTM_epochs, LSTM_val_loss, ':', label='Validation loss')\n",
    "plt.title('LSTM - Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "LSTM_model_acc_train = LSTM_model.evaluate(X_train, y_train, verbose=0)\n",
    "LSTM_model_acc_test = LSTM_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM - Training Accuracy: %.1f\" %  (LSTM_model_acc_train[1]*100))\n",
    "print(\"LSTM - Test Accuracy: %.1f\" %  (LSTM_model_acc_test[1]*100))\n",
    "print(\"LSTM - Loss: %.2f\" % LSTM_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------- Gated Recurrent Unit ------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "# define GRU model\n",
    "GRU_model = Sequential()\n",
    "# adding GRU layer\n",
    "GRU_model.add(GRU(200, input_shape=n_features[1:]))\n",
    "# adding fully connected layer\n",
    "GRU_model.add(Flatten())\n",
    "# adding output layer\n",
    "GRU_model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating early stopping and learning rate scheduler\n",
    "GRU_es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "GRU_lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "GRU_opt_adam = optimizers.Adam(learning_rate=0.001)\n",
    "GRU_model.compile(optimizer=GRU_opt_adam, loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model image\n",
    "tf.keras.utils.plot_model(GRU_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "GRU_hist = GRU_model.fit(X_train, y_train, epochs=40, batch_size=20, verbose=0, \n",
    "                 validation_data=(X_test, y_test), callbacks=[GRU_es, GRU_lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_acc = GRU_hist.history['accuracy']\n",
    "GRU_val = GRU_hist.history['val_accuracy']\n",
    "GRU_epochs = range(1, len(GRU_acc) + 1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(GRU_epochs, GRU_acc, '-', label='Training accuracy')\n",
    "plt.plot(GRU_epochs, GRU_val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "GRU_loss = GRU_hist.history['loss']\n",
    "GRU_val_loss = GRU_hist.history['val_loss']\n",
    "\n",
    "plt.plot(GRU_epochs, GRU_loss, '-', label='Loss')\n",
    "plt.plot(GRU_epochs, GRU_val_loss, ':', label='Validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "GRU_model_acc_train = GRU_model.evaluate(X_train, y_train, verbose=0)\n",
    "GRU_model_acc_test = GRU_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GRU - Training Accuracy: %.1f\" % (GRU_model_acc_train[1]*100))\n",
    "print(\"GRU - Testing Accuracy: %.1f\" % (GRU_model_acc_test[1]*100))\n",
    "print(\"GRU - Loss: %.2f\" % GRU_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------- Convolutional Neural Network --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "\n",
    "# define CNN model\n",
    "CNN_model = Sequential()\n",
    "# adding convolution layer\n",
    "CNN_model.add(Conv1D(128, kernel_size=4, activation='sigmoid', input_shape=n_features[1:], padding='same'))\n",
    "# adding pooling layer\n",
    "CNN_model.add(MaxPool1D(pool_size=2, padding='same'))\n",
    "# adding fully connected layer\n",
    "CNN_model.add(Flatten())\n",
    "# adding output layer\n",
    "CNN_model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating early stopping and learning rate scheduler\n",
    "CNN_es = EarlyStopping(monitor='val_loss', mode='min', verbose=10, patience=10)\n",
    "CNN_lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "CNN_opt_adam = optimizers.Adam(learning_rate=0.001)\n",
    "CNN_model.compile(optimizer=CNN_opt_adam, loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model image\n",
    "tf.keras.utils.plot_model(CNN_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "CNN_hist = CNN_model.fit(X_train, y_train, epochs=8, batch_size=35, verbose=0, \n",
    "                 validation_data=(X_test, y_test), callbacks=[CNN_es, CNN_lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_acc = CNN_hist.history['accuracy']\n",
    "CNN_val = CNN_hist.history['val_accuracy']\n",
    "CNN_epochs = range(1, len(CNN_acc) + 1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(CNN_epochs, CNN_acc, '-', label='Training accuracy')\n",
    "plt.plot(CNN_epochs, CNN_val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "CNN_loss = CNN_hist.history['loss']\n",
    "CNN_val_loss = CNN_hist.history['val_loss']\n",
    "\n",
    "plt.plot(CNN_epochs, CNN_loss, '-', label='Loss')\n",
    "plt.plot(CNN_epochs, CNN_val_loss, ':', label='Validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "CNN_model_acc_train = CNN_model.evaluate(X_train, y_train, verbose=0)\n",
    "CNN_model_acc_test = CNN_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN - Training Accuracy: %.1f\" % (CNN_model_acc_train[1]*100))\n",
    "print(\"CNN - Testing Accuracy: %.1f\" % (CNN_model_acc_test[1]*100))\n",
    "print(\"CNN - Loss: %.2f\" % CNN_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
